



The goal of the project is to allow PSRL to take advantage of the knowledge of an upper-bound on the span of the bias of the optimal policy (the regret then scales with this upper bound which can be much smaller than the diameter of the MDP). 

1) Please find attached an article describing the algorithm (more-efficient....pdf). The pseudo-code is on page 3. In this paper, they consider the finite horizon setting but we will consider the "average setting" (I am not sending the papers introducing PSRL in the average setting because they are more difficult to apprehend). But the algorithm will essentially be the same with minor modifications. I also attached UCRL2 algorithm (algo_ucrl2.pdf). For the implementation I recommend to keep PSRL structure but replace the second for-loop by the while loop of UCRL (see point 6 in algo_ucrl2.pdf). Also, since we are considering the average setting, \mu_k will be stationary (unlike in the finite horizon setting) and we will use value iteration to compute it (similarly to UCRL). Finally, we will use a normal distribution for the rewards and a Dirichlet distribution for the transitions. The updates rule for both distributions can be found here: https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions.

2) In the average setting, a policy is called "optimal" if it maximizes the long term average reward corresponding to the limit of 1/T*R(T) as T tends to +infinity (where T is the number of time steps and R(T) is the total expected reward after T steps). You can have a look at Chapter 8 of Puterman's book "Markov Decision Processes: Discrete Stochastic Dynamic Programming" (file chapter8.pdf attached). I recommend you to read the first 3/4 pages of the Chapter in order to have a better understanding of what the gain is, as well as section 8.2.1 entitled "Gain and Bias" where the "bias" of a policy is defined and some intuition is given. If you have time and if you are interested in the relationship between the discounted and average setting, you can also have a look at section 8.2.2 entitled "The Laurent series expansion" (the discount factor is denoted \lambda and not \gamma in this book). Finally, you can have a look at section 8.5.1 for value iteration in the average setting (pay attention to the stopping criterion).

3) You can start by implementing PSRL and test it on a simple MDP (3/4 states, like the one of the first TP). To take advantage of the prior knowledge on the bias span, we will only need to modify value iteration (and make minor changes in the main algorithm) so make sure value iteration is a separate function.

